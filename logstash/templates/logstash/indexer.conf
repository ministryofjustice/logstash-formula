{% from "logstash/map.jinja" import logstash with context %}

# based on: http://cookbook.logstash.net/recipes/syslog-pri/
input {
    tcp {
        port => 2514
        type => syslog
    }
    udp {
        port => 2514
        type => syslog
    }
    unix {
        path => "/data/logstash/indexer/logstash.sock"
        force_unlink => true
        codec => 'json_lines'
        data_timeout => 10
    }
    redis {
        host => '{{logstash.redis}}'
        data_type => list
        key => 'logstash:beaver'
        type => 'logstash:beaver'
    }
}

filter {
  if ! [@logstash_uuid] {
    uuid {
{% if logstash.version == '1.2.2' %}
      field => "@logstash_uuid"
{% else %}
      target => "@logstash_uuid"
{% endif %}
    }
  }
}


filter {
  if [type] == "syslog" {
    grok {
      patterns_dir => "/etc/logstash/patterns"
      match => { "message" => "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{host}" ]
    }
    syslog_pri { }
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
    if !("_grokparsefailure" in [tags]) {
      mutate {
        replace => [ "host", "%{syslog_hostname}" ]
        replace => [ "@message", "%{syslog_message}" ]
      }
    }
    mutate {
      remove_field => [ "syslog_hostname", "syslog_message", "syslog_timestamp" ]
    }
    if [message] =~ "IPTables" {
      grok {
        match => { "message" => "IPTables-Dropped: %{IPTABLES}"}
        patterns_dir => ["/etc/logstash/patterns"]
      }
    }
    date {
      match => [ "received_at", "yyyy-MM-dd HH:mm:ss z" ]
      target => "received_at"
    }
  }
}

# auditd and apparmor
filter {
  if 'audit' in [tags] {
    # If auditd is running apparmor will log to it.
    grok {
      patterns_dir => "/etc/logstash/patterns"
      match => [
                "message", "%{AUDIT}",
                "message", "%{AUDITLOGIN}",
                "message", "%{AUDITOTHER}",
                "message", "%{AUDIT_APPARMOR}"
               ]
    }
    kv {
      prefix => "audit_"
    }
    if [audit_type] == "AVC" {
      mutate {
        replace => ["tags", "apparmor"]
      }
    }
  }
  # If auditd is not running, Apparmor falls back to logging via syslog
  if [syslog_facility] == 'kernel' and [@message] =~ "apparmor" {
    grok {
      patterns_dir => "/etc/logstash/patterns"
      match => [ "message", "%{APPARMOR}" ]
    }
    kv {
      prefix => "syslog_apparmor_"
    }
    if [type] == '1400' {
      mutate {
        replace => ["tags", "apparmor"]
      }
    }
  }
}


output {

    file {
      path => "/data/logstash/indexer/archive.%{+YYYY-MM-dd}.json_lines.gz"
      codec => 'json_lines'
      gzip => true
    }

    statsd {
      increment => [
        "%{host}.indexer.events.per-type.%{type}",
        "%{host}.indexer.events.all"
      ]
      sender => "per-host"
    }

    statsd {
      increment => [
        "indexer.events.per-type.%{type}",
        "indexer.events.all"
      ]
      sender => "all"
    }

    # this should really be a type check but we're not good at assigning them
    # consistently.
    if [@fields][request_uri] {
      statsd {
        increment => [
          "%{host}.indexer.http_requests.by-status.%{[@fields][status]}",
          "%{host}.indexer.http_requests.all"
        ]
        sender => "per-host"
      }
      statsd {
        increment => [
          "indexer.http_requests.by-status.%{[@fields][status]}",
          "indexer.http_requests.all"
        ]
        sender => "all"
      }
    }

    elasticsearch_http {
        host => "{{logstash.elasticsearch.host}}"
        port => "{{logstash.elasticsearch.port}}"
        flush_size => 1
        document_id => "%{@logstash_uuid}"
    }

}
