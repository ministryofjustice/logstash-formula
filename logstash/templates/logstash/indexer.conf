{% from "logstash/map.jinja" import logstash with context %}

# based on: http://cookbook.logstash.net/recipes/syslog-pri/
input {
    tcp {
        port => 2514
        type => syslog
    }
    udp {
        port => 2514
        type => syslog
    }
    redis {
        host => '{{logstash.redis}}'
        data_type => list
        key => 'logstash:beaver'
        type => 'logstash:beaver'
    }
}


filter {
  if [type] == "syslog" {

    # It adds:
    # syslog_facility(_code)
    # syslog_severity(_code)
    # interestingly it does not add syslog_pri but that's ok, we have grog
    syslog_pri { }

    # syslog preprocessing
    # it will replace message and host with syslog entry and parse syslog header
    # supports both ubuntu default and new rsyslog format (later one is nicely precise)
    grok {
      match => [
        "message", "<%{POSINT:syslog_pri}>%{RSYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:host} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:message}",
        "message", "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:host} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:message}"
      ]
      patterns_dir => ["/etc/logstash/patterns"]
      overwrite => [ "message", "host" ]
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{host}" ]
    }

    # Let's use syslog server timestamp as an ordering timestamp
    # Parses rsyslog high precision timestamp (ISO8601)
    # Parses rsyslog low precition timestamp (both: 1 and 2 digits day variant)
    # Interestingly we can't expect date filter to automatically distinguish "UNIX" format
    date {
      match => [
        "syslog_timestamp",
        "ISO8601",
        "MMM dd HH:mm:ss",
        "MMM  d HH:mm:ss"
      ]
      remove_field => "syslog_timestamp"
    }

    # all syslog parsers in one place
    grok {
      match => [
        "message", "IPTables-Dropped: %{IPTABLES}",
        "message", "%{HAPROXYHTTP}",
        "message", "%{HAPROXYTCP}"
      ]
      patterns_dir => ["/etc/logstash/patterns"]
    }

    # lets split haproxy key=value pairs
    # use haproxy timestamp to sort events
    # tidy up kibana table view
    if [syslog_program] == "haproxy" {
      kv {
          source => "keyvalue"
          remove_field => "keyvalue"
      }
      date {
        match => [
            "accept_date", "dd/MMM/YYYY:HH:mm:ss.SSS"
        ]
        remove_field => "accept_date"
        remove_field => "haproxy_hour"
        remove_field => "haproxy_minute"
        remove_field => "haproxy_second"
        remove_field => "haproxy_milliseconds"
        remove_field => "haproxy_monthday"
        remove_field => "haproxy_month"
        remove_field => "haproxy_year"
        remove_field => "haproxy_time"
      }
    }
  }

  if 'audit' in [tags] {
    grok {
      patterns_dir => "/etc/logstash/patterns"
      match => [
        "message", "%{AUDIT}",
        "message", "%{AUDITLOGIN}",
        "message", "%{AUDITOTHER}"
      ]
    }
  }

  # TODO: align with other syslog groks
  if 'kernel' in [tags] {
    grok {
      patterns_dir => ["/etc/logstash/patterns"]
      match => [ "message", "%{APPARMOR}" ]
    }
    if '_grokparsefailure' not in [tags] {
      mutate {
        replace => ["tags", "apparmor"]
      }
    }
  }

  # TODO: consider flattening nginx logs so that we don't have @fields.xxx and i.e. we can easily filter logs
  # on request_id combining them from all different sources

  # following flattens just request_id (handy for events grouping in kibana)
  if [@fields] and [@fields][request_id] {
      mutate {
        replace => [ "request_id", "%{[@fields][request_id]}" ]
      }
  }

  # in case we pass high precision unix timestamp
  # i.e. used by nginx
  if [timestamp_msec] {
    date {
      match => [
          "timestamp_msec", "UNIX"
      ]
      remove_field => "timestamp_msec"
    }
  }
}


output {
    elasticsearch_http {
        host => "{{logstash.elasticsearch.host}}"
        port => "{{logstash.elasticsearch.port}}"
    }
}
